{
  "model_name_or_path": "Meta-Llama-3-8B",
  "torch_dtype": "bfloat16",
  "use_flash_attention_2": true,
  "peft_config": {
    "r": 8,
    "lora_alpha": 32,
    "lora_dropout": 0.1,
    "bias": "none",
    "task_type": "CAUSAL_LM"
  }
}
